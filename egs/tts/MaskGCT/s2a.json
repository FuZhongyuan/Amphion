{
    "model_type": "S2A",
    "dataset": {
        "libritts": 1.0
    },
    "preprocess": {
        "dataset_type": "libritts",
        "libritts_data_root": "/mnt/storage/fuzhongyuan_space/fzy_project/Amphion/data/LibriTTS",
        "libritts_cache_path": "/mnt/storage/fuzhongyuan_space/fzy_project/Amphion/data/LibriTTSCache",
        "hop_size": 320,
        "sample_rate": 16000,
        "max_length": 128000,
        "random_crop": true,
        "processed_dir": "",
        "valid_file": "valid.json",
        "train_file": "train.json",
        "load_semantic_features": true
    },
    "model": {
        "s2a_model_type": "full",
        "s2a_model": {
            "s2a_1layer": {
                "num_quantizer": 1,
                "hidden_size": 1024,
                "num_layers": 16,
                "num_heads": 16,
                "codebook_size": 1024,
                "cfg_scale": 0.15,
                "mask_layer_schedule": "linear",
                "cond_codebook_size": 8192,
                "cond_dim": 1024,
                "predict_layer_1": true,
                "gradient_checkpointing": true
            },
            "s2a_full": {
                "num_quantizer": 12,
                "hidden_size": 1024,
                "num_layers": 16,
                "num_heads": 16,
                "codebook_size": 1024,
                "cfg_scale": 0.15,
                "mask_layer_schedule": "linear",
                "cond_codebook_size": 8192,
                "cond_dim": 1024,
                "predict_layer_1": false,
                "gradient_checkpointing": true
            }
        },
        "semantic_codec": {
            "codebook_size": 8192,
            "hidden_size": 1024,
            "codebook_dim": 8,
            "vocos_dim": 384,
            "vocos_intermediate_dim": 2048,
            "vocos_num_layers": 12,
            "pretrained_path": "ckpts/maskgct/semantic_codec/checkpoint/epoch-XXXX_step-XXXXXXX_loss-X.XXXXXX/pytorch_model.bin",
            "representation_stat_mean_var_path": "models/tts/maskgct/ckpt/wav2vec2bert_stats.pt"
        },
        "acoustic_codec": {
            "encoder": {
                "d_model": 96,
                "up_ratios": [3, 4, 5, 8],
                "out_channels": 256,
                "use_tanh": false
            },
            "decoder": {
                "in_channels": 256,
                "upsample_initial_channel": 1536,
                "up_ratios": [8, 5, 4, 3],
                "num_quantizers": 12,
                "codebook_size": 1024,
                "codebook_dim": 8,
                "quantizer_type": "fvq",
                "quantizer_dropout": 0.5,
                "commitment": 0.25,
                "codebook_loss_weight": 1.0,
                "use_l2_normlize": true,
                "codebook_type": "euclidean",
                "kmeans_init": false,
                "kmeans_iters": 10,
                "decay": 0.8,
                "eps": 0.5,
                "threshold_ema_dead_code": 2,
                "weight_init": false,
                "use_vocos": true,
                "vocos_dim": 512,
                "vocos_intermediate_dim": 4096,
                "vocos_num_layers": 30,
                "n_fft": 1920,
                "hop_size": 480,
                "padding": "same"
            },
            "pretrained_path": "ckpts/maskgct/acoustic_codec/checkpoint/epoch-XXXX_step-XXXXXXX_loss-X.XXXXXX"
        },
        "representation_type": "w2v-bert-2.0",
        "representation_sample_rate": 16000,
        "use_norm_feat": true
    },
    "log_dir": "ckpts/maskgct",
    "train": {
        "max_epoch": 0,
        "use_dynamic_batchsize": false,
        "max_tokens": 16000000,
        "max_sentences": 200,
        "lr_warmup_steps": 10000,
        "lr_scheduler": "constant",
        "num_train_steps": 100000,
        "adam": {
            "lr": 1e-4,
            "betas": [0.5, 0.9]
        },
        "ddp": false,
        "random_seed": 114,
        "batch_size": 32,
        "epochs": 5000,
        "max_steps": 1000000,
        "total_training_steps": 800000,
        "save_summary_steps": 500,
        "save_checkpoints_steps": 2000,
        "valid_interval": 2000,
        "keep_checkpoint_max": 100,
        "gradient_accumulation_step": 1,
        "tracker": ["tensorboard"],
        "save_checkpoint_stride": [1],
        "keep_last": [10],
        "run_eval": [true],
        "dataloader": {
            "num_worker": 16,
            "pin_memory": true
        }
    }
}

