{
    "model_type": "S2MelDiT",
    "dataset": {
        "ljspeech": 1.0
    },
    "inference": {
        "t2s_n_timesteps": 60,
        "s2mel_n_timesteps": 50,
        "t2s_cfg": 1.0,
        "s2mel_cfg": 1.0,
        "use_ddim": true,
        "ddim_eta": 0.0
    },
    "preprocess": {
        "dataset_type": "ljspeech",
        "ljspeech_data_root": "data/LJSpeech-1.1",
        "ljspeech_cache_path": "data/LJSpeechCache",
        "hop_size": 320,
        "sample_rate": 16000,
        "n_fft": 1024,
        "num_mels": 80,
        "n_mel": 80,
        "win_size": 1024,
        "fmin": 0,
        "fmax": 8000,
        "mel_mean": 0.0,
        "mel_var": 1.0,
        "max_length": 128000,
        "random_crop": true,
        "processed_dir": "data/processed_maskgct/ljspeech",
        "valid_file": "valid.json",
        "train_file": "train.json",
        "load_semantic_features": true,
        "load_mel_spectrogram": true,
        "use_semantic_cache": true
    },
    "model": {
        "semantic_codec": {
            "codebook_size": 512,
            "hidden_size": 1024,
            "codebook_dim": 8,
            "vocos_dim": 384,
            "vocos_intermediate_dim": 2048,
            "vocos_num_layers": 12,
            "pretrained_path": "ckpts/maskgct_mini/semantic_codec_mini_160k_128/checkpoint/epoch-0170_step-0160000_loss-43.150742",
            "representation_stat_mean_var_path": "models/tts/maskgct/ckpt/wav2vec2bert_stats.pt"
        },
        "t2s_model": {
            "hidden_size": 384,
            "num_layers": 6,
            "num_heads": 6,
            "cfg_scale": 0.15,
            "cond_codebook_size": 512,
            "cond_dim": 256,
            "use_phone_cond": true,
            "gradient_checkpointing": false,
            "total_steps": 10000, //not max_step,because max_epoch is inf
            "stage1_steps": 0.3, //stage1_steps * total_steps
            "stage2_steps": 0.7,
            "min_mask_ratio": 0.1,
            "max_mask_ratio": 0.8
        },
        "s2mel_dit": {
            "mel_dim": 80,
            "hidden_size": 768,
            "num_layers": 12,
            "num_heads": 8,
            "cfg_scale": 0.15,
            "cond_codebook_size": 512,
            "num_diffusion_steps": 1000,
            "beta_start": 0.0001,
            "beta_end": 0.02,
            "beta_schedule": "linear",
            "gradient_checkpointing": false
        },
        "hifigan": {
            "resblock": "1",
            "upsample_rates": [10, 8, 2, 2],
            "upsample_kernel_sizes": [20, 16, 4, 4],
            "upsample_initial_channel": 512,
            "resblock_kernel_sizes": [3, 7, 11],
            "resblock_dilation_sizes": [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
        },
        "representation_type": "w2v-bert-2.0",
        "representation_sample_rate": 16000,
        "use_norm_feat": true
    },
    "log_dir": "ckpts/maskgct_mini",
    "train": {
        "max_epoch": 0,
        "use_dynamic_batchsize": true,
        "max_tokens": 8000000,
        "max_sentences": 50,
        "lr_warmup_steps": 10000,
        "lr_scheduler": "inverse_sqrt",
        "num_train_steps": 100000,
        "adam": {
            "lr": 1e-4,
            "betas": [0.9, 0.98],
            "weight_decay": 1.0e-4
        },
        "ddp": false,
        "random_seed": 114,
        "batch_size": 48,
        "epochs": 5000,
        "max_steps": 500000,
        "total_training_steps": 400000,
        "save_summary_steps": 500,
        "save_checkpoints_steps": 2000,
        "valid_interval": 2000,
        "keep_checkpoint_max": 50,
        "gradient_accumulation_step": 1,
        "tracker": ["tensorboard"],
        "save_checkpoint_stride": [1],
        "keep_last": [5],
        "run_eval": [true],
        "dataloader": {
            "num_worker": 8,
            "pin_memory": true,
            "prefetch_factor": 4
        }
    }
}
